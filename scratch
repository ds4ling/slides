
---
background-image: url(https://www.jvcasillas.com/media/rstats/memes/lm_additive_effects.png)
background-size: contain

---
background-image: url(../../assets/img/pensar2.png)
background-position: 95% 50%

### Overview

.Large[
- What if we based linguistics Grad School  
acceptances solely on GRE scores?
]

--

.Large[
- One shouldn't have to score high on all of  
the entrance requirements, but just rank  
high on the sum of all of them
]

--

.Large[
- We could use a weighted sum
]

---

### Overview

#### Weighted sum

.Large[
- Some things are more important than others in this overall sum

- For example, GRE is actually a poor predictor of graduate student success

- So we might want to weight this variable lower than something 
  like GPA or letters of recommendation

- We can use least squares estimation to get the b-weights with 
the lowest SS<sub>ERROR</sub>
]

---

### The equation

.pull-left[
.content-box-blue[
$$Y = a + bX + e$$
]
]

--

.pull-right[
.content-box-red[
$$Y = a_{0} + b_{1}X_{1}  + b_{2}X_{2} {...} b_{k}X_{k} + e$$
]
]

--

### Least Squares Estimation

- R will estimate the b-weights that minimize the sum of the squared errors 
(`r emojifont::emoji("no_good_woman")` by hand examples)

- Ideally these are the b-weights that best represent how much each predictor 
is really contributing to the variance in the criterion variable (y)

- We will again use least squares estimation to achieve the best (optimal) 
regression weights

---

### Multiple predictors

.Large[
- The multiple predictors represent different hypotheses regarding what 
might be affecting the criterion variable

- In other words, multiple regression is just creating a sum of weighted 
predictors to explain the total variance in the criterion variable

- The way that the predictors function together is not necessarily the 
same as the way that they each function alone

- Bivariate regression is just a degenerate form ("special case") of 
multiple regression containing only one predictor
]

---

### Interpretation

#### Summarizing so far...

.Large[
- In multiple regression we assume additivity and linearity

- In Boolean Algebra, a sum (addition) represents a logical disjunction

- The multiple regression "weighted sum" is a complex **OR** statement
]

--

</br>

### What does this mean for our parameter estimates and how do we interpret them?

---

### (Semi)partial betas

- .RUred[Intercept] ( $a_{0}$ ): the value of the criterion variable when all 
predictors are equal to 0  
(same as bivariate regression)

- .RUred[Slope] ( $b_{k}$ ): the change in the criterion associated with a 
1-unit change in $X_k$... 
--
*with all other predictors held constant*

--

</br>

### How can we calculate the unique contribution of each predictor?

---
background-image: url(./assets/img/ballentine1.png)
background-size: contain
background-position: 95% 50%

---
background-image: url(./assets/img/ballentine2.png)
background-size: contain
background-position: 95% 50%

--

.pull-right[
.footnote[
Recall that the b-weight of the bivariate model is $r(\frac{s_y}{s_x})$
]
]

---
background-image: url(./assets/img/ballentine3.png)
background-size: contain
background-position: 95% 50%

---
background-image: url(./assets/img/ballentine4a.png)
background-size: contain
background-position: 95% 50%

### The "Ballantine"

.footnote[Cohen & Cohen, 1975]

---
background-image: url(./assets/img/ballentine4b.png)
background-size: contain
background-position: 95% 50%

- $r^{2}_{x1,x2} = c + d$

--

- $r^{2}_{y,x1} = a + c$

--

- $r^{2}_{y,x2} = b + c$

--

- $R^{2}_{y,x1,x2} = a + b + c$

---
background-image: url(./assets/img/ballentine5.png)
background-size: contain
background-position: 95% 50%

### Squared semipartial correlation

- $X_2$ removed from *explained*  
variance

--

- Represents unique contribution  
of $X_1$

--

- $sr^2 = R^{2}_{y,x1,x2} - r^{2}_{y,x2}$

---
background-image: url(./assets/img/ballentine7.png)
background-size: contain
background-position: 95% 50%

### Squared partial</br>correlation

- Y variance not accounted for by  
$X_2$ is the area of A + E

--

- The area explained by $X_1$ = A  
(squared semipartial correlation)

--

- Unqiue contribution of $X_1$ is  
$A / (A + E)$ or...

--

- $pr^{2}_{x1} = \frac{A}{A + E}$

--

- We are pretending that $X_2$  
doesn't exist (statistically)

---
background-image: url(./assets/img/ballentine4b.png)
background-size: 300px
background-position: 95% 20%

### Statistical control<sup>1</sup>

.large[

<p></p>

- Not the same as .green[experimental control]
  - We are interested in some treatment
  - Some participants receive treatment, some do not
  - *Only* looking at highly proficient bilinguals (not low, med. prof.)

<p></p>

- We partial out the effects of predictor $X_1$ from all other predictors and 
use this error to determine $b_1$

- We are estimating the effect of one predictor while holding the other 
predictors constant

- This will only work if predictors are not correlated (i.e., there *cannot* be 
multicollinearity)

]

.footnote[<sup>1</sup>This is a poor choice for a name. Ask why.]

---
background-image: url(https://i.imgflip.com/25dwia.jpg)
background-position: 90% 50%
background-size: 450px

### Conceptual understanding

.pull-left[
.Large[
- MRC is much more difficult to conceptualize than the bivariate linear 
regression

- If we consider a simple three variable model (y ~ x<sub>1</sub> + 
x<sub>2</sub>) we are fitting a hyperplane to a three dimensional space

- More variables = more complexity
]
]

---
class: middle

```{r}
#| label: additive_plots
#| fig.width: 14
#| fig.height: 5

p1 <- ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point() + 
  geom_smooth(method = lm, formula = "y ~ x") +
  ds4ling_bw_theme(base_family = "Times", base_size = 16)

p2 <- ggplot(mtcars, aes(x = drat, y = mpg)) + 
  geom_point() + 
  geom_smooth(method = lm, formula = "y ~ x") +
  ds4ling_bw_theme(base_family = "Times", base_size = 16)

p1 + p2
```

--

.center[
.RUred[mpg ~ wt] & .blue[mpg ~ drat]  
!=  
MRC
]

---
background-color: black

<iframe src="./assets/html/threejs1.html" width='1000' height='500' style="border:none;"></iframe>

---

```{r}
#| label: additive_3d

# Left and right sides
plot3D::scatter3D(x, y, z, 
  pch = 21, cex = 1, expand = 0.75, colkey = F,
  theta = 0, phi = 5, ticktype = "detailed",
  xlab = "wt", ylab = "drat", zlab = "mpg", 
  col.panel ="steelblue")

plot3D::scatter3D(x, y, z, 
  pch = 21, cex = 1,  expand = 0.75, colkey = F,
  theta = 90, phi = 5, ticktype = "detailed",
  xlab = "wt", ylab = "drat", zlab = "mpg", 
  col.panel ="steelblue")
```

---

```{r}
#| label: additive_3d_surface

# Left and right sides with regression surface
plot3D::scatter3D(x, y, z, 
  pch = 21, cex = 1, expand = 0.75, colkey = F,
  theta = 0, phi = 5, ticktype = "detailed",
  xlab = "wt", ylab = "drat", zlab = "mpg",
  surf = list(x = x_pred, y = y_pred, z = z_pred_add,  
              facets = NA, col = 'grey60'))

plot3D::scatter3D(x, y, z, 
  pch = 21, cex = 1,  expand = 0.75, colkey = F,
  theta = 90, phi = 5, ticktype = "detailed",
  xlab = "wt", ylab = "drat", zlab = "mpg", 
  surf = list(x = x_pred, y = y_pred, z = z_pred_add,  
              facets = NA, col = 'grey60'))
```

---
background-image: url(./assets/img/additive_3d_corner.png)
background-size: contain

---
layout: true

# MRC

---

### Doing it in R

.pull-left[

```{r}
#| label: fit_mrc
mod <- lm(mpg ~ wt + drat, data = mtcars)
summary(mod)
```

]

.pull-right[

```{r}
#| label: fit_mrc_fake
#| eval: false
mod <- lm(mpg ~ wt + drat, data = mtcars) #<<
summary(mod)
```

]

---

### CIs and significance tests

```{r}
#| label: fit_mrc_print
cis <- confint(mod)
summary(mod) %>% 
  tidy(.) %>% 
  mutate(., `95% LB` = cis[, 1], `95% UB` = cis[, 2]) %>%
  dplyr::select(., ` ` = term, beta = estimate, SE = std.error, 
    `95% LB`, `95% UB`, `t-ratio` = statistic, p.value) %>% 
  kable(., format = 'html', digits = 2) %>% 
  kable_styling(., font_size = 22)
```

</br>

- Same as bivariate case, but we adjust t-value for k (added estimated 
parameters)

--

- Statistical significance implies that the 95% CI doesn't contain 0

--

- **Rule of thumb**: multiply SE of b-weight by 2 and add/subtract to/from 
b-weight  

--

- T-ratio: Parameter estimate divided by SE  

--
(i.e., 30.29 / 7.32 = `r round(30.29 / 7.32, 2)`)

--

- **Rule of thumb**: |t| > 2 = significant<sup>&trade;</sup> 

---

### Coefficient of multiple determination: R<sup>2</sup>

.large[
- Adding variables will always explain more variance

- Not necessarily better

- There is an adjustment for exhausting degrees of freedom
]

--

### Note

.large[
- .green[r]: pearson product moment correlation

- .purple[r<sup>2</sup>]: coefficient of determination; variance explained 
(bivariate case)

- .RUred[R<sup>2</sup>]: coefficient of multiple determination; variance 
explained (MRC)
]

---

### Making predictions

- Recall the multiple regression equation...

$$Y = a_{0} + b_{1}X_{1}  + b_{2}X_{2} {...} b_{k}X_{k} + e$$

- Our `mtcars` model can be summarized as...

```{r}
#| label: print-eq
extract_eq(mod, wrap = T, use_coefs = T, fix_signs = F, ital_vars = T)
```

--

- What is the predicted `mpg` for a car that weighs 1 unit with 
a rear axel ratio (drat) of 2?  
And one that weighs 1 with a drat of 4?  
And one that weighs 3 with a drat of 2.5?

--

  - `r round(coef(mod)[1] + (1 * coef(mod)[2]) + (2 * coef(mod)[3]), 2)` 
  $mpg = 30.29 + -4.78 \times 1 + 1.44 \times 2$

  - `r round(coef(mod)[1] + (1 * coef(mod)[2]) + (4 * coef(mod)[3]), 2)` 
  $mpg = 30.29 + -4.78 \times 1 + 1.44 \times 4$

  - `r round(coef(mod)[1] + (3 * coef(mod)[2]) + (2.5 * coef(mod)[3]), 2)` 
  $mpg = 30.29 + -4.78 \times 3 + 1.44 \times 2.5$

---
layout: false
class: title-slide-section-grey, middle

# Interactions

---
layout: true

# Interactions

---

#### Recall

.pull-left[

- Assumptions of Multiple Regression: 
  - Additivity
  - Linearity
- In Boolean Algebra, a sum (addition) represents a logical disjunction: 
  - Multiple regression "weighted sum" is a complex "OR" statement

]

.pull-right[

| A</br>(x) | B</br>(y) | C</br>(x **∨** y) |
| :-------: | :-------: | :---------------: |
| 0         | 0         |   **0**           | 
| 0         | 1         |   **1**           |
| 1         | 0         |   **1**           |
| 1         | 1         |   **1**           |

.center[

If A = 1  
**OR**  
B = 1,  
then C = 1  
Otherwise, C = 0

]
]

---

#### Recall

.pull-left[

- Assumptions of Multiple Regression: 
  - Additivity
  - Linearity
- In Boolean Algebra, a sum (addition) represents a logical .blue[disjunction]: 
  - Multiple regression "weighted sum" is a complex "OR" statement

#### Non-Additivity: Interaction Terms

- In Boolean Algebra, a product (multiplication) represents a logical 
**conjunction**:
  - Interaction terms represent "AND" terms
  - These are included within the overall "OR" statement

]

.pull-right[

| A</br>(x) | B</br>(y) | C</br>(x **∧** y) | </br>.grey[(x ∨ y)] |
| :-------: | :-------: | :---------------: | :-----------------: |
| 0         | 0         |   **0**           |   .lightgrey[0]     |
| 0         | 1         |   **0**           |   .lightgrey[1]     |
| 1         | 0         |   **0**           |   .lightgrey[1]     |
| 1         | 1         |   **1**           |   .lightgrey[1]     |

.center[

If A = 1  
**AND**  
B = 1,  
then C = 1  
Otherwise, C = 0

]
]

---

.pull-left[

### Genetics example

- You share 50% of genes with your Mom (M) and 50% with your Dad (D)

- But your parents don’t share that many genes

- M and D are generally not genetically correlated with each other, but you 
(the M\*D interaction) are correlated with both M and D (more on this later)

]

--

.pull-right[

### Drugs and alcohol

Consider taking the upcoming midterm in one of the following conditions

- Neither Drugs nor Alcohol:
  - probably best, produce highest score
- Alcohol alone:
  - Probably lower midterm scores than doing neither
- Drugs alone:
  - Probably lower midterm scores than doing neither
- Alcohol and Drugs together:
  - Probably lowest scores on the midterm exam of 
    all the four possible conditions

]

---

### Drugs and alcohol continued

<p></p>

.large[
- Is the effect of alcohol and drugs together equal to the negative effect 
of alcohol PLUS the negative effect of drugs?
  - Probably not
  - Alcohol and drugs are known to interact 
]

<p></p>

.large[
- This effect is a negative interaction
]

<p></p>

.large[
- If you mix alcohol and drugs they have a larger combined effect than 
adding up the effects of each acting separately:
  - For example, if alcohol makes your score drop 10 points, and the drug 
  drops it 15 points, when you take both your score will very likely drop 
  more than just 25 points
]

<p></p>

.large[
- This effect is not additive, it is multiplicative
]

---

### Non-additivity of effects

.Large[
- You cannot add the effect of alcohol to the effect of drugs to predict the effect "alcohol AND drugs"

- Because you get an extra effect (a boosting effect) by combing them

- There is a synergistic effect of combining the terms

- In principle, could be either more or less effective

- You include this multiplicative **AND** term (A\*D) in *ADDITION* to the other terms in the model
]

---

.pull-left[

### The multiple regression formula: 

$$Y = a_{0} + b_{1}X_{1} + b_{2}X_{2} + (b_{1}X_{1} \times b_{2}X_{2}) + e$$

### Including interactions in R

- We do this using `:`

- Or \*

]

--

.pull-right[

```{r}
#| label: nonadditive_mod
#| comment: ""
m <- lm(mpg ~ wt + drat + wt:drat, data = mtcars) #<<
summary(m)
```

]

---

### Visualization

.Large[
- Including an interaction affects the hyperplane fit to the data
]

---
background-color: black

```{r}
#| label: additive_interaction
# compare additive and int models
par(bg = 'black')
plot3D::scatter3D(x, y, z, 
  pch = 1, cex = 0, bty = "bl", expand = 0.75,
  theta = 50, phi = 10,  colkey = F, tick.col = 'white', 
  xlab = "wt", ylab = "drat", zlab = "mpg", 
  surf = list(x = x_pred, y = y_pred, z = z_pred_add,  
              facets = NA))

plot3D::scatter3D(x, y, z, 
  pch = 1, cex = 0, bty = "bl", expand = 0.75,
  theta = 50, phi = 10,  colkey = F,
  xlab = "wt", ylab = "drat", zlab = "mpg", 
  surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
              facets = NA))
```

---

```{r}
#| label: additive_interaction2

# compare additive and int models
#par(bg = 'black')
plot3D::scatter3D(x, y, z, 
  pch = 1, cex = 0, bty = "bl", expand = 0.75,
  theta = 50, phi = 10,  colkey = F,
  xlab = "wt", ylab = "drat", zlab = "mpg", 
  surf = list(x = x_pred, y = y_pred, z = z_pred_add,  
              facets = NA))

plot3D::scatter3D(x, y, z, 
  pch = 1, cex = 0, bty = "bl", expand = 0.75,
  theta = 50, phi = 10,  colkey = F,
  xlab = "wt", ylab = "drat", zlab = "mpg", 
  surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
              facets = NA))
```

---
background-color: black

```{r}
#| label: additive_interaction3

# compare additive and int models
par(bg = 'black')
plot3D::scatter3D(x, y, z, 
  pch = 1, cex = 0, bty = "bl", expand = 0.75,
  theta = 0, phi = 5,  colkey = F,
  xlab = "wt", ylab = "drat", zlab = "mpg", 
  surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
              facets = NA))

plot3D::scatter3D(x, y, z, 
  pch = 1, cex = 0, bty = "bl", expand = 0.75,
  theta = 90, phi = 5,  colkey = F,
  xlab = "wt", ylab = "drat", zlab = "mpg", 
  surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
              facets = NA))
```

---

```{r}
#| label: additive_interaction4

# compare additive and int models
plot3D::scatter3D(x, y, z, 
  pch = 1, cex = 0, bty = "bl", expand = 0.75,
  theta = 0, phi = 5,  colkey = F,
  xlab = "wt", ylab = "drat", zlab = "mpg", 
  surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
              facets = NA))

plot3D::scatter3D(x, y, z, 
  pch = 1, cex = 0, bty = "bl", expand = 0.75,
  theta = 90, phi = 5,  colkey = F,
  xlab = "wt", ylab = "drat", zlab = "mpg", 
  surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
              facets = NA))
```

---
layout: false
class: title-slide-section-red

# Review

### What we've seen so far

.Large[
- Bivariate regression
- MRC
- Additive effects
- Interactions (multiplicative effects)
]

### What's left

.Large[
- Assumptions
- Model specification
- Alpha slippage
- Empirical selection of variables
- Reporting results
]





















# {.transition visibility="uncounted"}

{{< tweet user=merm_bot id=1841652233651233066 >}}
<!-- another day coding in R --> 


<!-- https://www.react-graph-gallery.com/example/t-test-playground --> 

---

## {.smaller}

```{ojs}
// Import Observable Plot
import { Plot } from "@observablehq/plot";

// Create sliders for slope and intercept
viewof slope = Inputs.range([-5, 5], { 
  value: 1, 
  step: 0.1, 
  label: "Slope", 
  width: 250
});

viewof intercept = Inputs.range([-2, 2], { 
  value: 0, 
  step: 0.1, 
  label: "Intercept", 
  width: 250
});

// Stack sliders vertically
html`<div style="display: flex; flex-direction: column; gap: 20px; align-items: center;">
  <div>${viewof slope}</div>
  <div>${viewof intercept}</div>
</div>`;

// Generate a random dataset with 100 points
randomDataRaw = Array.from({ length: 100 }, () => {
  let x = Math.random() * 10;  // Random x values between 0 and 10
  let noise = (Math.random() - 0.5) * 2;  // Small noise for realism
  let y = x + noise;  // True relationship: y = x + noise
  return { x, y };
});

// Compute mean and standard deviation
meanX = randomDataRaw.reduce((sum, d) => sum + d.x, 0) / randomDataRaw.length;
meanY = randomDataRaw.reduce((sum, d) => sum + d.y, 0) / randomDataRaw.length;

stdX = Math.sqrt(randomDataRaw.reduce((sum, d) => sum + (d.x - meanX) ** 2, 0) / randomDataRaw.length);
stdY = Math.sqrt(randomDataRaw.reduce((sum, d) => sum + (d.y - meanY) ** 2, 0) / randomDataRaw.length);

// Standardize the data
fixedData = randomDataRaw.map(d => ({
  x: (d.x - meanX) / stdX,  // Standardized X
  y: (d.y - meanY) / stdY   // Standardized Y
}));

// Regression line updates dynamically, but data remains fixed
regressionLine = [
  { x: -2, y: slope * -2 + intercept },  // Line starts at x=-2
  { x: 2, y: slope * 2 + intercept }  // Line ends at x=2
]

// Create the scatterplot with fixed axes
Plot.plot({
  width: window.innerWidth,  // Full-width plot
  height: 750,
  marginLeft: 50,
  x: { 
    label: { text: "Standardized X", fontSize: 22 },  // Increase font size for x axis labels
    domain: [-2, 2],  // Fixed X-axis range
    tickFormat: (d) => d.toFixed(1)
  },
  y: { 
    label: { text: "Standardized Y", fontSize: 22 },  // Increase font size for y axis labels
    domain: [-2, 2],  // Fixed Y-axis range
    tickFormat: (d) => d.toFixed(1)
  },
  marks: [
    Plot.dot(fixedData, { x: "x", y: "y", fill: "steelblue", r: 3 }),  // Fixed scatter points
    Plot.line(regressionLine, { x: "x", y: "y", stroke: "red", strokeWidth: 2 })  // Regression line
  ]
})
```
