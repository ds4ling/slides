
---
layout: true

# Diagnostics

---

### Outliers/influential data points

- An influential point is one that would significantly change the fit if 
removed from the data 

- .blue[Cook’s distance] is a commonly used influence measure 

#### Leverage

- The leverage of an observation measures its ability to move the regression 
line by simply moving up/down along the y-axis.

- The measurement represents the amount by which the predicted value would 
change if the observation was shifted one unit in the y-direction.

- The leverage always takes values between 0 and 1.

- A point with 0 leverage does not effect the regression line.

---

### Influential data points

.pull-left[

```{r}
#| label: cooks_distance_linear
#| fig.height: 5.5
# autoplot(mod2, which = 4)
# Cook's D plot
cutoff <- function(model) {
  # identify D values > 4 / (n-k-1)
  n <- length(model$residuals)
  k <- length(model$coefficients)
  cutoff <- 4 / (n - k - 1)
  return(cutoff)
}

cooks.distance(mod1) %>% 
  tibble(., observation = as.numeric(names(.)), cooksd = .) %>%
  arrange(., observation) %>% 
  mutate(., label = if_else(cooksd > cutoff(mod1), 
                            true = as.character(observation), 
                            false = ' ')) %>% 
ggplot(., aes(x = observation, y = cooksd, label = label)) + 
  geom_hline(yintercept = cutoff(mod1), lty = 3) + 
  geom_point() + 
  geom_text(nudge_y = 0.1) + 
  geom_segment(aes(xend = observation, y = 0, yend = cooksd)) + 
  labs(x = "Obs. Number", y = "Cook's distance", title = "Cook's distance") + 
  ds4ling_bw_theme(base_family = "Times", base_size = 18)
```

]

.pull-right[

```{r}
#| label: cooks_distance_quadratic
#| fig.height: 5.5
cooks.distance(mod2) %>% 
  tibble(., observation = as.numeric(names(.)), cooksd = .) %>%
  arrange(., observation) %>% 
  mutate(., label = if_else(cooksd > cutoff(mod2), 
                            true = as.character(observation), 
                            false = ' ')) %>% 
ggplot(., aes(x = observation, y = cooksd, label = label)) + 
  geom_hline(yintercept = cutoff(mod2), lty = 3) + 
  geom_point() + 
  geom_text(nudge_y = 0.05) + 
  geom_segment(aes(xend = observation, y = 0, yend = cooksd)) + 
  labs(x = "Obs. Number", y = "Cook's distance", title = "Cook's distance") + 
  ds4ling_bw_theme(base_family = "Times", base_size = 18)
```

]

---

### Influential data points

```{r}
#| label: leverage_linear_quadratic2
#| fig.height: 5.5
#| fig.width: 12
#| fig.align: 'center'
ap3 <- autoplot(mod1, which = 5) + 
  ds4ling_bw_theme(base_family = "Times", base_size = 18)

ap4 <- autoplot(mod2, which = 5) + 
  ds4ling_bw_theme(base_family = "Times", base_size = 18)

ap3 + ap4
```

---

### Influential data points

.pull-left[

```{r}
#| label: mod1_with_outlier
#| fig.height: 6
noOut <- lm(y ~ x, data = assumptions_data[1:50, ])

assumptions_data %>% 
  mutate(., observation = row_number(),
            label = if_else(observation == 51, true = "51", false = " ")) %>% 
  ggplot(., aes(x = x, y = y, label = label)) + 
  geom_point(aes(color = label), size = 3, show.legend = FALSE) + 
  geom_text(nudge_y = 0.3, size = 5, color = "darkred") + 
  geom_smooth(method = lm, se = F, size = 1.5, color = "darkred", 
    formula = "y ~ x") + 
  geom_abline(intercept = noOut$coef[1], slope = noOut$coef[2], 
              color = 'blue', size = 1.5) + 
  scale_color_brewer(palette = "Set1", guide = FALSE, direction = -1) + 
  ds4ling_bw_theme(base_family = "Times", base_size = 18)
```

]

.pull-right[

#### .RUred[Mod1 with influential data point]:

.content-box-red[

```{r}
#| label: model_print_with_outlier
#| results: 'asis'
b_with <- coef(mod1)
cat(sprintf("$$y = %.02f + %.02f x$$", b_with[1], b_with[2]))
```

]

#### .blue[Mod1 without influential data point]: 

.content-box-blue[

```{r}
#| label: model_print_without_outlier
#| results: 'asis'
b_without <- coef(noOut)
cat(sprintf("$$y = %.02f + %.02f x$$", b_without[1], b_without[2]))
```

]
]

---

### Global test of model assumptions

.Large[
- It is also possible to use the package `gvlma` to test model assumptions. 

- It seems rather conservative

- I don't know too much about it. 
]

---

```{r}
#| label: gvlma_test_with
library(gvlma)
gvmodel_1with <- gvlma(mod1)
gvmodel_1with
```

---

```{r}
#| label: gvlma_test_without
gvmodel_1_without <- gvlma(noOut)
gvmodel_1_without 
```

---

```{r}
#| label: gvlma_test_mod2
gvmodel_2 <- gvlma(mod2)
gvmodel_2
```

---
layout: false
class: middle, center

<iframe src="https://gallery.shinyapps.io/slr_diag/" style="border:none;" height="600" width="1300"></iframe>

<!-- assumptions shiny app -->

---
layout: false
class: title-slide-section-grey, middle

# Interpretation

---
layout: true

# Interpretation

---

### Make sense of it all

.pull-left[

```{r}
#| label: model_output
#| comment: ''
cars_mod <- lm(mpg ~ wt, data = mtcars)
summary(cars_mod)
```

]

.pull-right[

1. Function call
2. Model residuals
3. Model coefficients
4. Significance codes
5. Variance explained
6. F-ratio

]

---

### Make sense of it all

.pull-left[

```{r}
#| label: model_output1
#| comment: ''
summary(cars_mod)$call
```

]

.pull-right[

1. Function .RUred[call]
2. Model residuals
3. Model coefficients
4. Significance codes
5. Variance explained
6. F-ratio

.content-box-blue[
- This is the model you fit using `lm()`. 
- It is the exact same code you typed into R. 
- Use to double check to see if you made any typos.
]

]

---

### Make sense of it all

.pull-left[

```{r}
#| label: model_output2
#| comment: ''
#| results: 'hold'
summary(cars_mod$residuals)
```

]

.pull-right[

1. Function call
2. Model .RUred[residuals]
3. Model coefficients
4. Significance codes
5. Variance explained
6. F-ratio

.content-box-blue[
- Should be normally distributed
- Absolute values of 1Q and 3Q should be similar
- Mean = 0, median close to 0
- If anything is off you can see it here, but check residual plots
]
]

---

### Make sense of it all

.pull-left[

```{r}
#| label: model_output3
#| comment: ''
#| results: 'hold'
summary(cars_mod)$coef
```

]

.pull-right[

1. Function call
2. Model residuals
3. Model .RUred[coefficients]
4. Significance codes
5. Variance explained
6. F-ratio

.content-box-blue[
- Meat and potatoes of the output
- Parameter estimates of intercept and predictor(s)
- Expresses strength of relationship between predictor(s) and outcome variable
- One unit change in predictor will change outcome by...
]
]

---

### Make sense of it all

.pull-left[

```{r}
#| label: model_output4
#| comment: ''
#| results: 'hold'
summary(cars_mod)
```

]

.pull-right[

1. Function call
2. Model residuals
3. Model coefficients
4. .RUred[Significance codes]
5. Variance explained
6. F-ratio

.content-box-blue[
- Assessment of statistical significance of predictor(s) and the intercept 
- Everything except "." is probably "good"
]
]

---

### Statistical Significance

.Large[
- As long as the effect is not statistically equivalent to 0 it is called 
statistically significant

- It may be an effect of trivial magnitude

- Basically, it means that this prediction is better than nothing

- It doesn’t really mean it is really “significant” in the terms that we 
  think of as significance

- It doesn’t indicate importance
]

---

### Significance versus Importance 

#### How do we know if the “significant” effect we found is actually important?

.large[
- The coefficient of determination (r-squared) tells you how much of the 
variance you have explained

- It tells us how big the effect is and not just that it is not equal to zero

- You want to know if your predictions are better than chance alone (F-ratio) 
but you also want to know how explanatory your predictions are (r-squared)
]

#### How important are the chosen predictors?

.large[
- The numerators in both ratios are similar

- They represent the predicted portion of the variance
]

---

### Make sense of it all

.pull-left[

```{r}
#| label: model_output5
#| comment: ''
#| results: 'hold'
summary(cars_mod)
```

]

.pull-right[

1. Function call
2. Model residuals
3. Model coefficients
4. Significance codes
5. .RUred[Variance explained]
6. F-ratio

.content-box-blue[
- Assessment of R<sup>2</sup>
- More variance explained is better
- Should pretty much always be reported
]
]

---

### Make sense of it all

.pull-left[

```{r}
#| label: model_output6
#| comment: ''
#| results: 'hold'
summary(cars_mod)
```

]

.pull-right[

1. Function call
2. Model residuals
3. Model coefficients
4. Significance codes
5. Variance explained
6. .RUred[F-ratio]

.content-box-blue[
- Assesses overall significance... omnibus model
- If F-ratio is significant, at least one predictor or intercept is too
- If F-ratio is not significant your experiment is over
]
]

---

### A note about F-ratios

#### If you have an ANOVA background... Mean Squared Deviations

$$MS_{Total} = \frac{\sum{(y_i - \bar{y})^2}}{n-1}$$  

$$MS_{Predicted} = \frac{\sum{(\hat{y}_i - \bar{y})^2}}{k}$$  

$$MS_{Error} = \frac{\sum{(y_i - \hat{y}_i)^2}}{n - k - 1}$$

.content-box-green[
#### $$F_{(k),(n-k-1)} = \frac{\sum{(\hat{y}_i - \bar{y})^2} / (k)}{\sum{(y_i - \hat{y}_i)^2} / (n - k - 1)}$$
]

---

### A note about F-ratios

#### If you have an ANOVA background... Mean Squared Deviations

$$MS_{Total} = \frac{SS_{Total}}{df_{Total}}$$

$$MS_{Predicted} = \frac{SS_{Predicted}}{df_{Predicted}}$$

$$MS_{Error} = \frac{SS_{Error}}{df_{Error}}$$

.content-box-green[
#### $$F = \frac{MS_{Predicted}}{MS_{Error}}$$
]

---

### Degrees of Freedom

#### Derived from the number of sample statistics used in your computation:

.Large[
- e.g., for a standard deviation, you subtract all the raw scores from the mean

- Since you used the sample mean, you used up 1 df

- df = n - 1
]

---

### Degrees of Freedom

#### In the denominator of the standard deviation, you are using a sample statistic, not a population parameter, so you have df = n - 1:

.large[
- n - 1 reflects the fact that you used a statistic and if you know one number 
(the mean) there is less uncertainty remaining
- If you know the mean and you have 10 scores, then you only need 9 of the 
remaining scores to predict all 10 of them
]

#### Usually we have an F-table with associated degrees of freedom to show us whether the F-ratio is "statistically significant":

.large[
- Numerator df = k 
- Denominator df = n - k - 1
]

---

### Degrees of Freedom

### $$df_{Total} = n - 1$$ 

### $$df_{Predicted} = k$$ 

### $$df_{Error} = n - k - 1$$

## $$df_{Total} = df_{Predicted} + df_{Error}$$

---
layout: false
class: middle

```{r}
#| label: tiktok2
tt_url2 <- "https://www.tiktok.com/@chelseaparlettpelleriti/video/6811647290709757189"
tt2 <- tiktok_embed(tt_url2)
tt2
```

--

.footnote[
[Exercises](https://www.ds4ling.jvcasillas.com/slides/05_lm/02_assumptions_diagnostics/assets/scripts/assumptions_walkthrough_ex.R)
]

---
layout: false
class: title-slide-final, left

# References

```{r}
#| label: load_refs
bib <- ReadBib(here("assets", "bib", "ds4ling_refs.bib"), check = FALSE)
ui <- "- "
```

```{r}
#| label: print_refs
#| results: 'asis'
writeLines(ui)
print(bib[key = "wickham2016r"], 
  .opts = list(check.entries = FALSE, 
               style = "html", 
               bib.style = "authoryear"))
writeLines(ui)
print(bib[key = "qass22_ch2"], 
  .opts = list(check.entries = FALSE, 
               style = "html", 
               bib.style = "authoryear"))
writeLines(ui)
print(bib[key = "qass57_ch1"], 
  .opts = list(check.entries = FALSE, 
               style = "html", 
               bib.style = "authoryear"))
```

- Figueredo, A. J. (2013).  Multiple Regression. *Statistical 
Methods in Psychological Research*.


















# {.transition visibility="uncounted"}

{{< tweet user=merm_bot id=1841652233651233066 >}}
<!-- another day coding in R --> 


<!-- https://www.react-graph-gallery.com/example/t-test-playground --> 

---

## {.smaller}

```{ojs}
// Import Observable Plot
import { Plot } from "@observablehq/plot";

// Create sliders for slope and intercept
viewof slope = Inputs.range([-5, 5], { 
  value: 1, 
  step: 0.1, 
  label: "Slope", 
  width: 250
});

viewof intercept = Inputs.range([-2, 2], { 
  value: 0, 
  step: 0.1, 
  label: "Intercept", 
  width: 250
});

// Stack sliders vertically
html`<div style="display: flex; flex-direction: column; gap: 20px; align-items: center;">
  <div>${viewof slope}</div>
  <div>${viewof intercept}</div>
</div>`;

// Generate a random dataset with 100 points
randomDataRaw = Array.from({ length: 100 }, () => {
  let x = Math.random() * 10;  // Random x values between 0 and 10
  let noise = (Math.random() - 0.5) * 2;  // Small noise for realism
  let y = x + noise;  // True relationship: y = x + noise
  return { x, y };
});

// Compute mean and standard deviation
meanX = randomDataRaw.reduce((sum, d) => sum + d.x, 0) / randomDataRaw.length;
meanY = randomDataRaw.reduce((sum, d) => sum + d.y, 0) / randomDataRaw.length;

stdX = Math.sqrt(randomDataRaw.reduce((sum, d) => sum + (d.x - meanX) ** 2, 0) / randomDataRaw.length);
stdY = Math.sqrt(randomDataRaw.reduce((sum, d) => sum + (d.y - meanY) ** 2, 0) / randomDataRaw.length);

// Standardize the data
fixedData = randomDataRaw.map(d => ({
  x: (d.x - meanX) / stdX,  // Standardized X
  y: (d.y - meanY) / stdY   // Standardized Y
}));

// Regression line updates dynamically, but data remains fixed
regressionLine = [
  { x: -2, y: slope * -2 + intercept },  // Line starts at x=-2
  { x: 2, y: slope * 2 + intercept }  // Line ends at x=2
]

// Create the scatterplot with fixed axes
Plot.plot({
  width: window.innerWidth,  // Full-width plot
  height: 750,
  marginLeft: 50,
  x: { 
    label: { text: "Standardized X", fontSize: 22 },  // Increase font size for x axis labels
    domain: [-2, 2],  // Fixed X-axis range
    tickFormat: (d) => d.toFixed(1)
  },
  y: { 
    label: { text: "Standardized Y", fontSize: 22 },  // Increase font size for y axis labels
    domain: [-2, 2],  // Fixed Y-axis range
    tickFormat: (d) => d.toFixed(1)
  },
  marks: [
    Plot.dot(fixedData, { x: "x", y: "y", fill: "steelblue", r: 3 }),  // Fixed scatter points
    Plot.line(regressionLine, { x: "x", y: "y", stroke: "red", strokeWidth: 2 })  // Regression line
  ]
})
```
