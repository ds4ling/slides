# Assumptions<br>(revisited) {background-image="https://raw.githubusercontent.com/jvcasillas/media/master/rstats/memes/lm_ballantine.png" background-size="contain" background-position="100% 50%" background-color="#272822"}

---

## A new assumption {.smaller background-image="./index_files/img/ballentine4b.png, ./index_files/img/ballentine8.png" background-size="650px, 650px" background-position="25% 90%, 75% 90%" data-menu-title="Multicollinearity"}

### Multicollinearity

- Multicollinearity occurs when the predictors are correlated
  - height (x<sub>1</sub>) and weight (x<sub>2</sub>)
  - intelligence (x<sub>1</sub>) and creativity (x<sub>2</sub>)

---

## {.smaller background-image="./index_files/img/ballentine4b.png, ./index_files/img/ballentine8.png" background-size="600px, 600px" background-position="95% 10%, 95% 90%"}

[A new assumption]{.emph .p-font style="font-size: 1.75em;"}

[Multicollinearity]{.p-font style="font-size: 1.2em; color: #666666;"}

::: {.columns}
::: {.column width="60%"}
- Why is it a problem?
  - "Confounds" produce ambiguity of causal inference
- Least Squares Estimation in Linear Model:
  - Simultaneous estimation of additive effects of all model predictors
  - Does not adequately partition variance among correlated predictors
- Least squares estimation is not adequate for dealing with multicollinearity
  - works best when predictors are uncorrelated
:::
:::

---

## {data-menu-title="Shiny app: Multicollinearity in MRC" background-iframe="https://gallery.shinyapps.io/collinearity/" background-interactive=TRUE}

::: notes
<https://gallery.shinyapps.io/collinearity/>
:::

<!-- Multicollinearity in multiple regression --> 

---

## Old assumptions revisited {.smaller}

### Model specification errors

#### [Avoid excluding relevant variables]{color="black"}

::: {.incremental}
- Use Multiple Working Hypotheses
- This is how you safeguard against leaving out a variable that you might need later
- But don't put in everything but the kitchen sink...[you also must avoid including irrelevant variables]{.fragment}
- Ideally you want to use plausible rival hypotheses
- You need to have some theory behind your reasoning
- Assuming you have done all of this, we need to figure out which predictors 
are relevant and which are not relevant
- This is not nearly as a big of a problem as having excluded a relevant 
variable
:::

---

## {.smaller}

[How useful is the model output?]{.emph .p-font style="font-size: 1.75em;"}

[Traditional t-test to determine if the b = 0]{.p-font style="font-size: 1.2em; color: #666666;"}

::: {.closelist}
- Take b-weight, compute standard error
- Use SE<sub>b</sub> and the b-weight and construct a t-ratio:

$$t_{b} = \frac{b}{SE_{b}}$$

- Theoretically this informs us of whether b = 0 or not
:::

. . .

[Problem]{.p-font style="font-size: 1.2em; color: #666666;"}

::: {.closelist}
- The b-weights and the SE of the b-weight are both critically dependent 
on the model being correctly specified!
- If you made either kind of model specification error, the parameter estimates are invalid
  - The b-weights might have changed if you omitted a relevant variable
  - The SE of the b-weights will have changed (increased) if you included an irrelevant variable
:::

---

## {.smaller background-image="../../assets/img/frustracion.jpg" background-position="95% 20%" background-size="350px"}

[How useful is the model output?]{.emph .p-font style="font-size: 1.75em;"}

[Your t-tests might not be informative]{.p-font style="font-size: 1.2em; color: #666666;"}

::: {.columns}
::: {.column width="70%" .incremental}
- If the model is not correctly specified then it is based on incorrect values of either b or SE<sub>b</sub> or both
- If you knew the model was correctly specified then you would have no need to use the t-tests in the first place
- You only need to use them if you're unsure whether the model is correctly specified
- So the only conditions in which the tests are useful are the conditions in which they may be invalid!
- But you only test this if you are questioning the specification of the model
:::
:::

. . .

[How do you deal with this?]{.p-font style="font-size: 1.2em; color: #666666;"}

::: note
Hierarchical partitioning of variance through nested model comparisons
:::




# Hierarchical Partitioning of Variance {.transition}

---

## Hierarchical Partitioning of Variance {data-menu-title="Nested model comparisons" background-image="./index_files/img/turtles.png" background-position="90% 50%" background-size="500px"}

### Nested model comparisons

::: {.columns}
::: {.column}
- Comparison of "hierarchically nested" multiple regression models
- Requires theoretical specification and causal ordering of predictors
:::
:::

---

## Nested model comparisons {data-menu-title="What is a nested model?" background-image="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTvWty12X7seRPCGTHaZRccZNzwuoRHfxSGrHdjz69zugf5Ux6c01Mhp07bRjgxzghwTJA&usqp=CAU" background-position="50% 95%" background-size="550px"}

### What is a nested model?

::: box-note
A nested model is when one model is nested inside the other such that there is 
a more inclusive model that contains more parameters and a less inclusive model 
(restricted model) that contains just a subset of specific variables you would 
like to test
:::

---

##

[Nested model comparisons]{.emph .p-font style="font-size: 1.75em;"}

[What is a nested model?]{.p-font style="font-size: 1.2em; color: #666666;"}

::: {.closelist style="font-size: 0.85em;"}
- In nested model comparisons, you are testing whether those parameters not in the restricted model can be eliminated:
  - You want to see if those parameters not in the restricted models can be set to 0
  - You want to see if these extra parameters are needed or if they can be taken out
:::

. . .

[How]{.emph .p-font style="font-size: 1.2em; color: #666666;"}

- Test a more complicated model and then a less complicated one

---

## {data-menu-title="Comparing nested models"}

[Nested model comparisons]{.emph .p-font style="font-size: 1.75em;"}

### Comparing nested models

::: {.closelist}
- You often cannot compare two different restricted models directly
- If the models do not overlap they cannot be directly compared
- You must construct an inclusive model such that both restricted models are nested within a common inclusive model
- Then, to do the nested model comparisons you run the alternative restricted models and test each against the same inclusive model
:::
